\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{listingsutf8}

\title{Języki i Narzędzia Programowania 2 - CUDA - Duże zadanie zaliczeniowe.}
\begin{document}
    \maketitle
    \section{Wstęp}
    Dany jest system informacyjny składający się z wielu obiektów, z których każdy jest opisany zmiennymi opisowymi oraz zmienną decyzyjną. Celem jest zidentyfikowanie zmiennych które są powiązane ze zmienną decyzyjną.
    \section{Entropia informacyjna}
    Entropia informacyjna to miara nieprzewidywalności informacji - dla danego zbioru zdarzeń przyjmuje największą wartość, gdy wszystkie są równie prawdopodobne. Wyraża się wzorem
    \begin{equation} H(X) = E[I(X)], \end{equation}
gdzie $X$ to dana zmienna losowa, $E$ - operator wartości oczekiwanej, a $I(X)$ - zawartość informacyjna zmiennej określona jako
    \begin{equation} I(X) = -\log_r(P(X)). \end{equation}
$P(X)$ to funkcja masy prawdopodobieństwa , a $r$ w naszych zastosowaniach będzie równe 2 - otrzymana entropia informacyjna będzie mierzona w bitach. Tak więc, explicite nasza entropia (dla zbioru zdarzeń $A$) będzie wyrażać się wzorem
    \begin{equation} H_A = -\sum\limits_{a\in A} p_a \log_2 p_a, \end{equation}
gdzie $p_a$ jest równe prawdopodobieństwu zdarzenia $a$.
    \section{Przyrost informacji dla zmiennych}
    Tutaj informacje z Wikipedii i to co jest w dokumencie z zadaniem nie klei mi się do końca. Wzór z dokumentu wydaje się być uproszczony, ale 
    Intuicyjnie: zmienna $X_{ij}$ jest sprzężona z $D_i$ jeśli po jej ustaleniu entropia informacyjna zmniejsza się w stosunku do sumy entropii dla $X_{ij}$ i $D_i$ osobno - pewne kombinacje wartości są bardziej prawdopodobne od innych. Ustalmy więc wzór:
    \begin{equation} IG(B) = H_A +H_B - H_{A\times B}, \end{equation}
    w którym $A$ to zbiór zdarzeń, że $D_i$ jest równe pewnej wartości, a $B$ to zbiór zdarzeń że dane ustalone zmienne $X_{ij}$ są równe pewnym wartościom. Wtedy największy przyrost informacji między jedną a dwoma zmiennymi to:
    \begin{equation} GIG_{a}(x,y) = IG(x,y) - \max(IG(x), IG(y)). \end{equation}
    \section{Treść zadania}
    Wyznaczyć P par zmiennych, dla których GIG jest największy. Bonus: trzy zmienne. Wariant II: zmienne są rzeczywiste i należy sprawdzać wiele różnych dyskretyzacji.
    \section{Generacja danych testowych}
    Aby efektywnie testować swoje rozwiązanie na dowolnym rozmiarze danych wejściowych postanowiłem generować własne uproszczone testy. Program testmaker najpierw losuje wszystkie zmienne opisowe dla obiektów, a potem gdy wartość zmiennej/pary zmiennych zgadza się z jedną z podanych par, zwiększa szanse na to że obiekt "będzie chory". Nie jest to idealne odwzorowanie rzeczywistości - częstość mutacji nie jest tak duża a choroby także dużo rzadsze ale bardzo upraszcza to program.
    Przy jego pomocy wygenerowałem testy z 1 - 30 000 obiektów i 1 - 20 000 000 zmiennych, przy czym te większe zajmowały ok. 1 GB i czas działania był za długi żeby cokolwiek tym testować.
    \section{Rozwiązanie}
    Do policzenia przyrostu informacji potrzebujemy dla każdej pary zmiennych wiedzieć ile obiektów jest danej klasy, gdzie klasa obiektu jednoznacznie określa jakie wartości danych zmiennych on posiada. Możemy to przedtawić jako tabelkę 3 x 3 (x 2).

    \begin{tabular}{|l|r|r|r|}
        \hline
        chorzy & 0 & 1 & 2 \\
        \hline
        0 & & & \\
        \hline
        1 & & & \\
        \hline
        2 & & & \\
        \hline
    \end{tabular}
    \begin{tabular}{|l|r|r|r|}
        \hline
        zdrowi & 0 & 1 & 2 \\
        \hline
        0 & & & \\
        \hline
        1 & & & \\
        \hline
        2 & & & \\
        \hline
    \end{tabular}

    Nie unikniemy liczenia liczebności tych klas dla każdej pary osobno - jest to zdecydowanie zbyt dużo danych do policzenia "wcześniej": dla każdej pary zmiennych (20 000 000 x 20 000 000 x 0.5) potrzebujemy tabelki z 18 liczebnościami klas (18 zmiennych short). To jest 200 000 000 000 000 * 18 * 2 B = 7 200 000 000 000 000 B = 7 200 TB.

    \begin{lstlisting}
        for kazda para zmiennych (a, b):
            for kazdy obiekt o z O:
                zalicz go do swojej klasy;
            policz GIG;
            zapisz pare jesli GIG > threshold;
    \end{lstlisting}

    Punktem krytycznym wydaje się być policzenie liczebności klas zmiennych - wymaga przejścia przez wszystkie obiekty, a więc przejrzenie dużej ilości pamięci. Policzenie GIG na podstawie zliczonych klas nie powinno zajmować dużo cyklów, ponieważ liczebności klas będą już w rejestrach multiprocesora.

    Aby zmniejszyć ilość pamięci, na każdą zmienną opisową przeznaczyłem tylko 2 bity. W ten sposób mogą one posłużyć jako indeks do tablicy zliczającej klasy.

    \begin{lstlisting}
        ...
    \end{lstlisting}

    Ponieważ dostępy do pamięci z różnych wątków powinny być obok siebie, rzędy tablicy odpowiadają obiektom - w ten sposób, gdy 32 wątki na raz czytają dane obiektu x, obszar pamięci o który proszą pamięć główną jest spójny.

    Naturalnym pomysłem jest wykorzystanie pamięci shared do przyspieszenia programu. Przy 30 000 obiektów do 48kB zmieści się tam ok. 6,5 zmiennej - jest to trochę mało więc lepiej wczytywać obiekty na bieżąco w trakcie liczenia. Z jakiegoś powodu nie przyspiesza to programu. Lepsze efekty daje zwiększenie rozmiaru cache L1.


    Ogólna struktura programu:
    \begin{lstlisting}
        wygeneruj losowe permutacje obiektow i zmiennych;
        wczytaj dane;
        uruchom kernel dla pierwszych ~10\% zmiennych;
        posortuj wyniki;
        ustal poziom odciecia;
        uruchom docelowy kernel dla wszystkich zmiennych;
        ew. posortuj wyniki i je wypisz;
    \end{lstlisting} 
    Aby nie przerzucać danych między RAMem a kartą moglibyśmy rozważyć wybieranie poziomu odcięcia na karcie. Nie jest to jednak czasochłonne

\end{document}
