\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{listingsutf8}

\title{Języki i Narzędzia Programowania 2 - CUDA - Duże zadanie zaliczeniowe.}
\begin{document}
    \maketitle
    \section{Wstęp}
    Dany jest system informacyjny składający się z wielu obiektów, z których każdy jest opisany zmiennymi opisowymi oraz zmienną decyzyjną. Celem jest zidentyfikowanie zmiennych które są powiązane ze zmienną decyzyjną.
    \section{Treść zadania}
    Wyznaczyć P par zmiennych, dla których GIG jest największy. Bonus: trzy zmienne. Wariant II: zmienne są rzeczywiste i należy sprawdzać wiele różnych dyskretyzacji.
    \section{Generacja danych testowych}
    Aby efektywnie testować swoje rozwiązanie na dowolnym rozmiarze danych wejściowych napisałem generator uproszczonych testów. Program testmaker najpierw losuje wszystkie zmienne opisowe dla obiektów, a potem gdy wartość zmiennej/pary zmiennych zgadza się z jedną z podanych par, zwiększa szanse na to że obiekt "będzie chory". Nie jest to idealne odwzorowanie rzeczywistości - częstość mutacji nie jest tak duża a choroby także dużo rzadsze ale bardzo upraszcza to program.

    Przy jego pomocy wygenerowałem testy z 1 - 30 000 obiektów i 1 - 20 000 000 zmiennych, przy czym te większe zajmowały ok. 1 GB i czas działania był za długi żeby cokolwiek tym testować. Dołączam testerkę do paczki razem z plikami generującymi oraz wynikami czasowymi poszczególnych testów.
    \section{Rozwiązanie}
    Do policzenia przyrostu informacji potrzebujemy dla każdej pary zmiennych wiedzieć ile obiektów jest danej klasy, gdzie klasa obiektu jednoznacznie określa jakie wartości danych zmiennych on posiada. Możemy to przedtawić jako tabelkę 3 x 3 (x 2).

    \begin{tabular}{|l|r|r|r|}
        \hline
        chorzy & 0 & 1 & 2 \\
        \hline
        0 & & & \\
        \hline
        1 & & & \\
        \hline
        2 & & & \\
        \hline
    \end{tabular}
    \begin{tabular}{|l|r|r|r|}
        \hline
        zdrowi & 0 & 1 & 2 \\
        \hline
        0 & & & \\
        \hline
        1 & & & \\
        \hline
        2 & & & \\
        \hline
    \end{tabular}

    Nie unikniemy liczenia liczebności tych klas dla każdej pary osobno - jest to zdecydowanie zbyt dużo danych do policzenia "wcześniej": dla każdej pary zmiennych (20 000 000 x 20 000 000 x 0.5) potrzebujemy tabelki z 18 liczebnościami klas (18 zmiennych short). To jest 200 000 000 000 000 * 18 * 2 B = 7 200 000 000 000 000 B = 7 200 TB.

    \begin{lstlisting}
        for kazda para zmiennych (a, b):
            for kazdy obiekt o z O:
                zalicz go do swojej klasy;
            policz GIG;
            zapisz pare jesli GIG > threshold;
    \end{lstlisting}

    Punktem krytycznym wydaje się być policzenie liczebności klas zmiennych - wymaga przejścia przez wszystkie obiekty, a więc przejrzenie dużej ilości pamięci. Policzenie GIG na podstawie zliczonych klas nie powinno zajmować dużo cyklów, ponieważ liczebności klas będą już w rejestrach multiprocesora. Istotnie, gdy w końcowym programie wykomentowałem większość obliczeń, nie zauważyłem wzrostu wydajności.

    Aby zmniejszyć ilość pamięci, na każdą zmienną opisową przeznaczyłem tylko 2 bity. W ten sposób mogą one posłużyć jako indeks do tablicy zliczającej klasy.

    \begin{lstlisting}
for (int i = 0; i < num_objects; ++i) {
    int d = bfe(ds[i / 32], i % 32, 1);
    int v1 = bfe(vars[i * vars_width + v1_p / 16], (v1_p % 16) * 2, 2);
    int v2 = bfe(vars[i * vars_width + v2_p / 16], (v2_p % 16) * 2, 2);
    count[d][v1][v2]++;
}
    \end{lstlisting}

    Tablica count[~][~][~] jest typu int - co ciekawe zmiana na short zwiększa ilość rejestrów potrzebnych kernelowi i spowalnia program.

    Istnieje w PTX specjalna instruckcja do manipulacji bitfieldami (BFE), ale ponieważ wydajność jest ograniczona (jak zgaduję) przez pamięć, niewiele ona nam daje.

    Ponieważ dostępy do pamięci z różnych wątków powinny być obok siebie, rzędy tablicy odpowiadają obiektom - w ten sposób, gdy 32 wątki na raz czytają dane obiektu x, obszar pamięci o który proszą pamięć główną jest spójny.

    Naturalnym pomysłem jest wykorzystanie pamięci shared do przyspieszenia programu. Przy 30 000 obiektów do 48kB zmieści się tam ok. 6,5 zmiennej - jest to trochę mało więc lepiej wczytywać obiekty na bieżąco w trakcie liczenia. Z jakiegoś powodu nie przyspiesza to programu. Podejrzewam że w zły sposób odwołuję się do shared'a, ale nie mogę dokładnie ustalić co się dzieje. Lepsze efekty daje zwiększenie rozmiaru cache L1. Wersja c11 używa pamięci shared a wersja c12 zwiększa rozmiar cache L1.

    Ogólna struktura programu:
    \begin{lstlisting}
        wygeneruj losowa permutacje zmiennych;
        wczytaj dane;
        uruchom kernel dla pierwszych 10% zmiennych;
        posortuj wyniki;
        ustal poziom odciecia;
        uruchom docelowy kernel dla wszystkich zmiennych;
        ew. posortuj wyniki i je wypisz;
    \end{lstlisting} 
    Abu nie przepisywać kilka razy tablicy zmiennych, na początku programu generuję permutację 10\% zmiennych i wczytuję je do pamięci już pomieszane.

    Aby nie przerzucać danych między RAMem a kartą moglibyśmy rozważyć wybieranie poziomu odcięcia na karcie. W niektórych wypadkach przepisanie i sortowanie tablicy GIG z randomizowanej próby zajmuje 30\% czasu działania. Wymagałoby to jednak napisania algorytmu sortowania/Hoare'a na kartę lub skopiowanie jakiegoś z internetu. Pomyślałem że nie jest to główna część zadania więc postawiłem na prostotę.

    Dane są wypisywane posortowane aby ułatwić testowanie. Niestety jestem prawie pewny że gdzieś mam jeszcze błąd bo wyniki dwóch wersji się różnią...

\end{document}
